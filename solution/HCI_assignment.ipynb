{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f0fc5d85df5ead7",
   "metadata": {},
   "source": [
    "### Beginning of the Assignment - exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40caebbd49c3aae4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T17:52:36.557235Z",
     "start_time": "2025-12-15T17:52:36.553813Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "#pd.set_option('max_columns', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bb707aefb5cf4",
   "metadata": {},
   "source": [
    "### PHIL DATA SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5b4d2ef82f5903",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T15:29:47.424479Z",
     "start_time": "2025-12-12T15:29:47.385479Z"
    }
   },
   "outputs": [],
   "source": [
    "character_nicknames_df = pd.read_csv('datasetscharacter_nicknames.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69c4d7be6f9b0da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T15:29:50.753199Z",
     "start_time": "2025-12-12T15:29:50.349512Z"
    }
   },
   "outputs": [],
   "source": [
    "details_df = pd.read_csv('datasets/details.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b151dad2401364",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T15:29:55.783610Z",
     "start_time": "2025-12-12T15:29:54.424922Z"
    }
   },
   "outputs": [],
   "source": [
    "favs_df = pd.read_csv('datasets/favs.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e121b1e7955869bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T15:29:58.376706Z",
     "start_time": "2025-12-12T15:29:58.098291Z"
    }
   },
   "outputs": [],
   "source": [
    "person_details_df = pd.read_csv('datasets/person_details.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1624a96bf99276",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T15:29:59.342905Z",
     "start_time": "2025-12-12T15:29:59.316905Z"
    }
   },
   "outputs": [],
   "source": [
    "person_alternate_names_df = pd.read_csv('datasets/person_alternate_names.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698cb4f95d8af93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T15:30:04.166140Z",
     "start_time": "2025-12-12T15:30:03.825481Z"
    }
   },
   "outputs": [],
   "source": [
    "person_anime_works_df = pd.read_csv('datasets/person_anime_works.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3aa40b33b894e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T15:30:05.161751Z",
     "start_time": "2025-12-12T15:30:05.079030Z"
    }
   },
   "outputs": [],
   "source": [
    "stats_df = pd.read_csv('datasets/stats.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8396e73e4803de",
   "metadata": {},
   "source": [
    "### DENIS DATA SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f1f790c14be4a88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T15:31:34.795537Z",
     "start_time": "2025-12-12T15:30:07.077041Z"
    }
   },
   "outputs": [],
   "source": [
    "ratings_df = pd.read_csv('datasets/ratings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76032cb1462fb43b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T15:33:17.731522Z",
     "start_time": "2025-12-12T15:33:15.851275Z"
    }
   },
   "outputs": [],
   "source": [
    "characters_df = pd.read_csv('datasets/characters.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ea38d0ed1db7ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T15:33:23.906707Z",
     "start_time": "2025-12-12T15:33:23.780769Z"
    }
   },
   "outputs": [],
   "source": [
    "character_anime_works_df = pd.read_csv('datasets/character_anime_works.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cda4e6b11746a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T15:33:24.754306Z",
     "start_time": "2025-12-12T15:33:24.570871Z"
    }
   },
   "outputs": [],
   "source": [
    "person_voice_works_df = pd.read_csv('datasets/person_voice_works.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdb2c596fa23862",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T18:10:44.766961Z",
     "start_time": "2025-12-15T18:10:44.243869Z"
    }
   },
   "outputs": [],
   "source": [
    "profiles_df = pd.read_csv('datasets/profiles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a462170b7d3912c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T15:33:26.386872Z",
     "start_time": "2025-12-12T15:33:26.356003Z"
    }
   },
   "outputs": [],
   "source": [
    "recommendations_df = pd.read_csv('datasets/recommendations.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4376ce",
   "metadata": {},
   "source": [
    "# GUIDELINES\n",
    "### BEFORE STARTING\n",
    "Use Conda in order to do the correct setup\n",
    "\n",
    "When we deliver the project, we need to tell the lecturer to run the jupyter notebook before running the TWEB part. How? Write it in the Report\n",
    "Give instructions in order to make it run properly\n",
    "\n",
    "#TODO to write on the report:\n",
    "Which are the cool things, problems, map names not allined so we had to normalize them and so on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcce46ca",
   "metadata": {},
   "source": [
    "Things to do for each dataset:\n",
    "1. Give it a look with .head and/or .tail\n",
    "2. .describe and check if all the numeric values make sense (e.g. year=300 makes no sense in our context)\n",
    "3. Check the format: objects to date if we need it. Check if all the dates are in the same format: us or eu\n",
    "    also check if there's any 29/02/2013. It doesn't exist right? Maybe this is too much lol\n",
    "4. Check for duplicates\n",
    "5. CHECK FOR CORRELATION: df.corr() (e.g. with longer duration, there are more actors)\n",
    "#TODO\n",
    "6. ADD or Remove columns?\n",
    "7. GROUPBY selects the elements and makes group out of it, combines the numeric fields of each specific group\n",
    "#TODO We could use it grouping for language and looking at how many anime are made in japan, stating it's the first country where the culture of doing (and watching) anime is SO big\n",
    "8. Aggregations: we can apply multiple different aggregated functions (e.g. for the first column you sum the data, for the second you do the average and so on)\n",
    "9. Transformations: apply operations and return results aligned with the original DF\n",
    "\n",
    "10. Removing NaNs is wrong in general because Pandas will skip it.\n",
    "We do it when? Is it safe to remove NaNs rows if EVERY field in the row is empty? I hope so lol\n",
    "BE CAREFUL if they are foreign keys: for example, if a person has a nan in the \"anime he worked in\" field, it shouldn't be dropped\n",
    "NEVER replace with invalid values (e.g. -1)\n",
    "IF we use df.dropna(subset=[\"name\"],inplace=True)\n",
    "the inplace means that the df itself is modified and will result in the one without the na. Without \"inplace=true\" you'll need to assign the result to another df (or the same) \n",
    "\n",
    "\n",
    "Proviamo i plot? df.plot()\n",
    "\n",
    "11. Check if data are consistent (e.g. normalizing names of countries and/or numeric fields, describing them and checking what they are)\n",
    "\n",
    "12. Normalize data types all in the same place (e.g. all the dates in the same cell)\n",
    "\n",
    "\n",
    "BONUS: NEVER USE LOOP FOR, NEVER DUPLICATE DATA (unless necessary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559aaa934a6028f2",
   "metadata": {},
   "source": [
    "##### First look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b52901143474e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T18:09:51.570461Z",
     "start_time": "2025-12-03T18:09:51.546798Z"
    }
   },
   "outputs": [],
   "source": [
    "character_nicknames_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdaecd75792d0bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T18:09:51.715797Z",
     "start_time": "2025-12-03T18:09:51.681944Z"
    }
   },
   "outputs": [],
   "source": [
    "character_nicknames_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f2346d047cbc84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T18:09:51.883862Z",
     "start_time": "2025-12-03T18:09:51.873486Z"
    }
   },
   "outputs": [],
   "source": [
    "character_nicknames_df.columns\n",
    "#will list all the columns. Not necessary here but kept as a pattern to follow with the following files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a166976d6165f4c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T18:09:51.944933Z",
     "start_time": "2025-12-03T18:09:51.935225Z"
    }
   },
   "outputs": [],
   "source": [
    "character_nicknames_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4eda35e9a19e60",
   "metadata": {},
   "source": [
    "### Data preparation (cleaning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71262e4b352d9474",
   "metadata": {},
   "source": [
    "##### On the first dataset we may need to check for duplicates.\n",
    "What does that mean? We have 102 rows that are duplicated over a 37080 rows dataset.\n",
    "Why is that? Are there some characters that have multiple nicknames, so they are repeated in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92969c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "character_nicknames_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0631ab9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "character_nicknames_df.describe(include='all')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f256644d9df068f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T15:33:40.422416Z",
     "start_time": "2025-12-12T15:33:40.299153Z"
    }
   },
   "outputs": [],
   "source": [
    "character_nicknames_df.loc[character_nicknames_df.duplicated()]\n",
    "#by default will give us the second"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ba43973017cba6",
   "metadata": {},
   "source": [
    "Mhh they're somehow different so yeah, the same character could have different nicknames.\n",
    "We want to drop though the ones that are exactly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344037bd6d62638b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T18:09:54.980944Z",
     "start_time": "2025-12-03T18:09:54.950703Z"
    }
   },
   "outputs": [],
   "source": [
    "#this way we drop the duplicates on the first dataset\n",
    "\n",
    "# OLD VERSION \n",
    "# character_nicknames_df = character_nicknames_df.loc[~character_nicknames_df.duplicated()].reset_index(drop=True).copy()\n",
    "\n",
    "#we don't need to use a subset here because there are just 2 columns\n",
    "\n",
    "#why don't we just use drop_duplicates()?\n",
    "character_nicknames_df.drop_duplicates(keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ca6149ba118158",
   "metadata": {},
   "source": [
    "Let's check for nan values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6168da3a85323cf0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T18:09:59.373588Z",
     "start_time": "2025-12-03T18:09:59.359680Z"
    }
   },
   "outputs": [],
   "source": [
    "character_nicknames_df[character_nicknames_df.isna().any(axis=1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bfce02f7b6df3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T18:10:00.080752Z",
     "start_time": "2025-12-03T18:10:00.067960Z"
    }
   },
   "outputs": [],
   "source": [
    "#cleaning the df from nan values\n",
    "character_nicknames_df.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f036600d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD VERSION\n",
    "# character_nicknames_df.to_csv(\"datasets/character_nicknames_cleaned.csv\", index=False, line_terminator=\"\\n\")\n",
    "# With this command, the cleaned dataset is bigger than the original\n",
    "# This is because on Windows line endings occupy 2 bytes instead of 1\n",
    "# In order to fix it and have an actual smaller file that matches with macOS / Linux, we add the argument \n",
    "# lineterminator, which in older versions of pandas may not work, and must be replaced with line_terminator\n",
    "character_nicknames_df.to_csv(\"datasets/character_nicknames_cleaned.csv\", index=False, lineterminator=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76b3aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO put this somewhere else\n",
    "small_datasets = {\n",
    "    \"character_nicknames\": character_nicknames_df,\n",
    "    \"details\": details_df,\n",
    "    \"person_alternate_names\": person_alternate_names_df,\n",
    "    \"person_details\": person_details_df,\n",
    "    \"stats\": stats_df,\n",
    "    \"ratings\": ratings_df,\n",
    "    \"characters\": characters_df,\n",
    "    \"person_voice_works\": person_voice_works_df,\n",
    "    \"profiles\": profiles_df,\n",
    "}\n",
    "\n",
    "for name, df in small_datasets.items():\n",
    "    df.to_csv(f\"{name}_cleaned.csv\", index=False, lineterminator=\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ecacd990a27e5b",
   "metadata": {},
   "source": [
    "## SECOND DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec30ca445b47f67",
   "metadata": {},
   "source": [
    "##### On the second dataset we may need to check for missing values and/or inconsistent values, since there are no duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9e440942e0bc9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T15:51:33.992685Z",
     "start_time": "2025-12-12T15:51:33.937995Z"
    }
   },
   "outputs": [],
   "source": [
    "details_df\n",
    "#anime details\n",
    "#japanes title could be dropped?\n",
    "#members stand for how many users have this anime added to their list.\n",
    "#explicit_genres is empty so can be removed\n",
    "#licensor and streaming are mostly empty. Do we care?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fd3fd28c2741f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to see what are \"type\"\n",
    "details_df['type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b138de4f13369292",
   "metadata": {},
   "outputs": [],
   "source": [
    "details_df.loc[details_df.duplicated(subset=['url'], keep='first')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d31f89c6105ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "details_df.query('year>2025')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5895d17ad81645",
   "metadata": {},
   "outputs": [],
   "source": [
    "details_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3968075d4de60cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "details_df[['start_date','season']].query(\"season.notna()\")\n",
    "#season can be removed? Do we care about the season? We can \"calculate\" it from the \"start_date\" field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6aa49e2052e976c",
   "metadata": {},
   "outputs": [],
   "source": [
    "details_df.query(\"episodes > 2500\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385d854ee6b9da6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "details_df[[\"start_date\", \"end_date\"]] = details_df[[\"start_date\", \"end_date\"]].apply(\n",
    "    pd.to_datetime, errors=\"coerce\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c760cdab33dc03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "details_df[[\"scored_by\", \"rank\", \"episodes\", \"year\"]] = (\n",
    "    details_df[[\"scored_by\", \"rank\", \"episodes\", \"year\"]].astype(\"Int64\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8ab91e14a99bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "details_df.dtypes\n",
    "#scored_by, rank, episodes, year can be an int instead of a float\n",
    "#start and end dates are not objects but dates\n",
    "#do we need to swap the empty [] with Nan or not? WE should in order to be able to use the .isna() method and other pandas methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6bcd8d159d2711",
   "metadata": {},
   "source": [
    "### THIRD DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6482f5884fe7415d",
   "metadata": {},
   "outputs": [],
   "source": [
    "favs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cb47295a56f1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to see what are \"fav_type\"\n",
    "favs_df['fav_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60745a14aa28b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "favs_df.isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e310ab028ae5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "favs_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847b46866e24884a",
   "metadata": {},
   "outputs": [],
   "source": [
    "favs_df.duplicated().sum()\n",
    "#there are no duplicates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7a4654641134d5",
   "metadata": {},
   "source": [
    "### FOURTH DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92462999a9ad06ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_alternate_names_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bed1ff9faa6c1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_alternate_names_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4fbdfaff7fc108",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_alternate_names_df.isna().sum()\n",
    "person_alternate_names_df[person_alternate_names_df.isna().any(axis=1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29d9f6f81c3461f",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_alternate_names_df.dropna(inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d51a9e80efc1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# person_alternate_names_df.loc[person_alternate_names_df['person_mal_id'].duplicated()]\n",
    "person_alternate_names_df[person_alternate_names_df.duplicated(subset=['person_mal_id','alt_name'], keep=False)].sort_values(['person_mal_id','alt_name'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b8a9ad9f84a4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_alternate_names_df.drop_duplicates(keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4b9da82ba09c53",
   "metadata": {},
   "source": [
    "### FIFTH DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43304e3b2f8067d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_details_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efd1309a834162b",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_details_df.loc[person_details_df['person_mal_id'].duplicated()]\n",
    "#we found that the duplicates differ for the \"relevant_location\" field, which has no interest for us so we drop the duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e928415c99210775",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_details_df.drop_duplicates(subset=['person_mal_id', 'url', 'name'], keep='first', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d92e7520dfb544",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_details_df.dtypes\n",
    "#we need to change birthday from object to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994211cb9f67f0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_details_df[\"birthday\"] = pd.to_datetime(person_details_df[\"birthday\"], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b630223a2ab165",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_details_df[\"birthday\"].min(), person_details_df[\"birthday\"].max()\n",
    "#makes sense because they're just composers of used music in anime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf1e3ccd4f377a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_details_df.isna().sum()\n",
    "#we have to check the nan values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777d73883a4885ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_details_df[person_details_df[\"name\"].isna()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96597e9889f1c36",
   "metadata": {},
   "source": [
    "We can join the two tables person_details_df and person_alternate_names_df having the keys that match.\n",
    "Putting the alternate names in a new column called alt_name and having a list of those inside"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646233103f95bf88",
   "metadata": {},
   "source": [
    "### SIXTH DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8a185495c0fac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_anime_works_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0985945f44ea99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_anime_works_df.dtypes\n",
    "#the types are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898138dd7fe4dc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_anime_works_df.isna().sum()\n",
    "#There's no nan value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ad2cfe2be526dd",
   "metadata": {},
   "source": [
    "### SEVENTH DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf7e3a021ac0845",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df.filter(regex=\"_votes$\").astype(\"Int64\")\n",
    "stats_df[stats_df.filter(regex=\"_votes$\").columns] = (\n",
    "    stats_df.filter(regex=\"_votes$\").astype(\"Int64\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39a3de40a42ce78",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df.isna().sum()\n",
    "#there are 430 series without any votes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8375f8980f805a",
   "metadata": {},
   "source": [
    "### EIGHTH DATASET "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81148221c3ef57cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cfecd02cabce78",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f6a516eb227ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have to understand the sense of \"num_watched_episodes\" and the link with \"is_rewatching\"\n",
    "ratings_df.query('is_rewatching == 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3befb102a25b244d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5ce74ce49118f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change \"is_rewatching\" from float to Int8, to save memory\n",
    "ratings_df[\"is_rewatching\"] = ratings_df[\"is_rewatching\"].astype(\"Int8\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b120acc6ab51b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df[[\"anime_id\",\"score\",\"num_watched_episodes\"]].agg([\"min\", \"max\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ef0a28383f6b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to save up some memory, we can change \"anime_id\" from Int64 to Int32 because there are no anime with id > 2,147,483,647\n",
    "ratings_df[\"anime_id\"] = ratings_df[\"anime_id\"].astype(\"Int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648ea850",
   "metadata": {},
   "outputs": [],
   "source": [
    "#same with \"score\", which goes from 1 to 10\n",
    "ratings_df[\"score\"] = ratings_df[\"score\"].astype(\"Int8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe998b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#same with \"num_watched_episodes\", which can't go over 2 billions\n",
    "ratings_df[\"num_watched_episodes\"] = ratings_df[\"num_watched_episodes\"].astype(\"Int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16e5c89288a2f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df[ratings_df.duplicated(subset=['username','anime_id'], keep=False)].sort_values(['username','anime_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e55904174dc326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# usually we should drop all the occurrence of a duplicate and keep the first\n",
    "# in this case though, it looks like the latest occurence is the most updated one, contaning more info than the first one, so we drop the first one\n",
    "ratings_df.drop_duplicates(subset=['username', 'anime_id'], keep='last', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce1bf5a74967db2",
   "metadata": {},
   "source": [
    "We had just 6 duplicates having the same username and anime_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cb137cda84d449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we check for Nan values.\n",
    "#TODO\n",
    "# if it is necessary check if the num_watched_episodes is greater than number of episodes of anime, we can remove the Nan values and put one or zero. \n",
    "ratings_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32609afbb47253b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop \"username\" with Nan values?\n",
    "#TODO\n",
    "ratings_df[ratings_df['username'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898fc17a38038251",
   "metadata": {},
   "source": [
    "Check this username that there is in the profiles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a64ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_chunk = True\n",
    "for chunk in pd.read_csv(\"datasets/ratings.csv\", chunksize=2_000_000):\n",
    "    chunk[\"is_rewatching\"] = chunk[\"is_rewatching\"].astype(\"Int8\")\n",
    "    chunk[\"anime_id\"] = chunk[\"anime_id\"].astype(\"Int32\")\n",
    "    chunk[\"score\"] = chunk[\"score\"].astype(\"Int8\")\n",
    "    chunk[\"num_watched_episodes\"] = chunk[\"num_watched_episodes\"].astype(\"Int32\")\n",
    "\n",
    "    chunk.to_csv(\n",
    "        \"datasets/ratings_half_cleaned.csv\", mode=\"w\" if first_chunk else \"a\", \n",
    "        index=False, header=first_chunk,lineterminator=\"\\n\"\n",
    "    )\n",
    "    first_chunk = False\n",
    "\n",
    "ratings_cleaned_df = pd.read_csv(\"datasets/ratings_half_cleaned.csv\")\n",
    "ratings_cleaned_df.drop_duplicates(subset=[\"username\", \"anime_id\"], keep=\"last\", inplace=True)\n",
    "ratings_cleaned_df.to_csv(\"datasets/ratings_cleaned.csv\", index=False, lineterminator=\"\\n\")\n",
    "\n",
    "# This takes sooo long I'm not sure it's worth it. More than 12 minutes on my most powerful machine.\n",
    "# The cleaned version is about the same size as the original one and we save just a bit of memory when loading it (10% less)\n",
    "# This is because the file will be read and written twice, just for a small gain in memory usage\n",
    "# Why twice? Because if we drop duplicates whithin each chunk while reading it, we may miss duplicates that are in different chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d23298c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "    \"is_rewatching\": \"Int8\",\n",
    "    \"anime_id\": \"Int32\",\n",
    "    \"score\": \"Int8\",\n",
    "    \"num_watched_episodes\": \"Int32\",\n",
    "}\n",
    "\n",
    "ratings = pd.read_csv(\"datasets/ratings.csv\", dtype=dtypes, low_memory=False)\n",
    "\n",
    "ratings[\"username\"] = ratings[\"username\"].astype(\"category\")\n",
    "\n",
    "ratings.drop_duplicates(subset=[\"username\", \"anime_id\"], keep=\"last\", inplace=True)\n",
    "\n",
    "ratings.to_csv(\"datasets/ratings_cleaned.csv\", index=False, lineterminator=\"\\n\")\n",
    "\n",
    "# converted high-cardinality string columns with many repeated values \n",
    "# (e.g., username) to the category type to reduce memory usage.\n",
    "# This representation stores each distinct value once and references it via integer \n",
    "# codes, allowing the dataset to be processed efficiently on machines with limited RAM.\n",
    "# THIS \"ONLY\" TAKES 5 MINUTES TO RUN\n",
    "# It actually causes more ram usage, not sure why. Is it because of the conversion to category? \n",
    "# Or because we try to do all these operations in the same cell?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e590dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2fd009e3cb2f1d55",
   "metadata": {},
   "source": [
    "### NINTH DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f132c9c5b439865b",
   "metadata": {},
   "outputs": [],
   "source": [
    "characters_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf77859b4c1fea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check types of dataset columns\n",
    "characters_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b90d91c87325e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change \"character_mal_id\" and \"favorites\" from float to int\n",
    "characters_df[\"character_mal_id\"] = characters_df[\"character_mal_id\"].astype(\"Int64\")\n",
    "characters_df[\"favorites\"] = characters_df[\"favorites\"].astype(\"Int64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262b59d9e7ab712f",
   "metadata": {},
   "outputs": [],
   "source": [
    "characters_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51ea147dbb548de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have only 2 rows where all columns are Nan, the rows with Nan values in \"name_kanji\" and \"about\" we shouldn't drop because they have other values that are important.\n",
    "characters_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83998fbda1510ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we want to check if the Nan values are concentrate in only two rows\n",
    "characters_df[characters_df['character_mal_id'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edf07a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "characters_df[characters_df['name_kanji'].isna()]\n",
    "# TODO could we analyze how a missing kanji states something about the character? 1/4 of the characters don't have it\n",
    "# I guess that kanji name is missing for non-Japanese characters and/or for minor characters.\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec922906053b3b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apart \"name_kanji\" and \"about\" the others Nan values are concentrate in two rows so we drop the two rows with all columns Nan\n",
    "characters_df.dropna(how='all', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e740ccc88a1dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to see all duplicates to understand if we have to drop or not\n",
    "characters_df.loc[characters_df.duplicated(subset=['character_mal_id', 'url', 'name'], keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e396358a0b99436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we drop the duplicates because they have all same values \n",
    "characters_df.drop_duplicates(subset=['character_mal_id', 'url', 'name'], keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5c4ec82f24619",
   "metadata": {},
   "source": [
    "### TENTH DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9a7e6f14d4576d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# role of character anime\n",
    "character_anime_works_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa5d2f0fdf5098b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check types of columns\n",
    "character_anime_works_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb229ef9b49bc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the number of Nan value\n",
    "character_anime_works_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fc95f21626ff18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the number of duplicates\n",
    "character_anime_works_df.loc[character_anime_works_df.duplicated(subset=['anime_mal_id', 'character_mal_id'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24326514f39c0ae5",
   "metadata": {},
   "source": [
    "There is no need to clean this dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8ade7b84a24dc7",
   "metadata": {},
   "source": [
    "### ELEVENTH DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdc70d72f90897",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_voice_works_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb8ca8a047c4d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_voice_works_df['language'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c809d5c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T12:42:38.087189500Z",
     "start_time": "2025-12-12T12:41:09.172172Z"
    }
   },
   "outputs": [],
   "source": [
    "person_voice_works_df['language'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89551dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I want to group by language and see the different languages available in the dataset.\n",
    "person_voice_works_df.groupby('language').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205eaacb00fff92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_voice_works_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63783dc49ac28a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_voice_works_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9839d0c9d8d719bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the duplicates are in all columns\n",
    "person_voice_works_df.loc[person_voice_works_df.duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc6b8c4e4eb2778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the duplicates because they have all same values\n",
    "person_voice_works_df.drop_duplicates(keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a409aef3a0a83a9",
   "metadata": {},
   "source": [
    "### TWELFTH DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842fdcde8d97a4f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T18:12:25.801898Z",
     "start_time": "2025-12-15T18:12:25.788989Z"
    }
   },
   "outputs": [],
   "source": [
    "# Should we delete the last five columns?\n",
    "# TODO\n",
    "profiles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6850cf22694f24bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the types are right for each field\n",
    "profiles_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f1434b0fc3b4b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T18:12:48.446264Z",
     "start_time": "2025-12-15T18:12:48.438124Z"
    }
   },
   "outputs": [],
   "source": [
    "# trying things with the date\n",
    "# BEFORE running the .to_datetime command, ishiyama_yumi has Jul 24 bday\n",
    "profiles_df.loc[44]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e0cfca8b069e17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T18:13:34.540997Z",
     "start_time": "2025-12-15T18:13:33.626977Z"
    }
   },
   "outputs": [],
   "source": [
    "# change types of columns \"birthday\" and \"joined\" from object to date and the others columns that they should be int\n",
    "profiles_df[\"birthday\"] = pd.to_datetime(profiles_df[\"birthday\"], errors='coerce')\n",
    "profiles_df[\"joined\"] = pd.to_datetime(profiles_df[\"joined\"], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0fc56b5221543",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T18:13:44.448038Z",
     "start_time": "2025-12-15T18:13:44.439862Z"
    }
   },
   "outputs": [],
   "source": [
    "# AFTER running the .to_datetime command, ishiyama_yumi has NaT birthday\n",
    "profiles_df.loc[44]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04e07a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T17:54:53.832104Z",
     "start_time": "2025-12-15T17:54:53.815141Z"
    }
   },
   "outputs": [],
   "source": [
    "profiles_df[\"birthday\"].min(), profiles_df[\"birthday\"].max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed71d85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T18:12:36.808122Z",
     "start_time": "2025-12-15T18:12:36.752673Z"
    }
   },
   "outputs": [],
   "source": [
    "weird_birthdays = profiles_df[\n",
    "    (profiles_df[\"birthday\"] < \"1900-01-01\") |\n",
    "    (profiles_df[\"birthday\"] > \"2025-12-31\")\n",
    "]\n",
    "\n",
    "weird_birthdays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a1c0ab057e16f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo\n",
    "# Just noticed some birthdays have no year but just day and month, how could we manage those?\n",
    "# I guess with errors='coerce', when we change type from object to date, the year is set to a default one (e.g. 1800 or 1900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31373a79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T18:06:05.239853Z",
     "start_time": "2025-12-15T18:06:05.222917Z"
    }
   },
   "outputs": [],
   "source": [
    "profiles_df.loc[\n",
    "    profiles_df[\"birthday\"] > profiles_df[\"joined\"],\n",
    "    [\"birthday\", \"joined\"]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2684b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T17:59:00.359899Z",
     "start_time": "2025-12-15T17:59:00.348188Z"
    }
   },
   "outputs": [],
   "source": [
    "profiles_df[\"joined\"].min(), profiles_df[\"joined\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f39e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles_df[\"birthday\"].dt.year.value_counts().sort_index().head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42635655",
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles_df[profiles_df[\"birthday\"].dt.year == 1930]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa073380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decided to remove the birthdays of people older than 100 when they joined the website\n",
    "mask_too_old = (profiles_df[\"joined\"] - profiles_df[\"birthday\"]).dt.days / 365.25 > 100\n",
    "profiles_df.loc[mask_too_old, \"birthday\"] = pd.NaT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a136f7df882ddf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T18:04:03.164988Z",
     "start_time": "2025-12-15T18:04:03.137712Z"
    }
   },
   "outputs": [],
   "source": [
    "mask_too_young = (profiles_df[\"joined\"] - profiles_df[\"birthday\"]).dt.days / 365.25 < 3\n",
    "mask_too_young\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff1027fe1e58bcf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T18:05:53.834818Z",
     "start_time": "2025-12-15T18:05:53.717091Z"
    }
   },
   "outputs": [],
   "source": [
    "profiles_df.isna().sum()\n",
    "#TODO one guy has null username\n",
    "# We'll put a progressive unkown_user[] label on it, so that for future data with missing username we could go on with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fb1286ddcf52d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T18:04:23.586648Z",
     "start_time": "2025-12-15T18:04:23.524413Z"
    }
   },
   "outputs": [],
   "source": [
    "# check if there is any duplicate on \"username\"\n",
    "profiles_df.loc[profiles_df.duplicated(subset=['username'], keep='first')]\n",
    "# none found"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad49ed6002b15b0",
   "metadata": {},
   "source": [
    "### THIRTEENTH DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afea57a30ef472",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0c3eddd66192f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c73b87f02bb7ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T16:29:26.196165Z",
     "start_time": "2025-12-04T16:29:26.153263Z"
    }
   },
   "outputs": [],
   "source": [
    "recommendations_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e511bdd1cc10a283",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T16:29:29.076395Z",
     "start_time": "2025-12-04T16:29:28.829775Z"
    }
   },
   "outputs": [],
   "source": [
    "recommendations_df.loc[profiles_df.duplicated(keep='first')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe886e7",
   "metadata": {},
   "source": [
    "# TO KEEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c363c691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st\n",
    "# dropping duplicates\n",
    "character_nicknames_df.drop_duplicates(keep='first', inplace=True)\n",
    "# dropping nan values\n",
    "character_nicknames_df.dropna(inplace=True)\n",
    "\n",
    "# 2nd\n",
    "# no need to clean from duplicates nor missing values\n",
    "# change column types from object to datetime\n",
    "details_df[[\"start_date\", \"end_date\"]] = details_df[[\"start_date\", \"end_date\"]].apply(\n",
    "    pd.to_datetime, errors=\"coerce\"\n",
    ")\n",
    "# change column types from object to Int64\n",
    "details_df[[\"scored_by\", \"rank\", \"episodes\", \"year\"]] = (\n",
    "    details_df[[\"scored_by\", \"rank\", \"episodes\", \"year\"]].astype(\"Int64\")\n",
    ")\n",
    "\n",
    "# 3rd\n",
    "# no need to clean\n",
    "\n",
    "\n",
    "# 4th\n",
    "#dropping nan values because they don't give any useful information\n",
    "person_alternate_names_df.dropna(inplace=True)\n",
    "# dropping duplicates\n",
    "person_alternate_names_df.drop_duplicates(keep='first', inplace=True)\n",
    "\n",
    "\n",
    "# 5th\n",
    "# Dropping duplicates choosing to keep the first occurrence. They only differ for the \"relevant_location\" field which has no interest for us\n",
    "person_details_df.drop_duplicates(subset=['person_mal_id', 'url', 'name'], keep='first', inplace=True)\n",
    "# change column types from object to datetime\n",
    "person_details_df[\"birthday\"] = pd.to_datetime(person_details_df[\"birthday\"], errors='coerce')\n",
    "\n",
    "# There are two weird rows with nan values in most columns but we don't drop them because they may be linked to other datasets\n",
    "# Could we join the two tables person_details_df and person_alternate_names_df having the keys that match.\n",
    "# Putting the alternate names in a new column called alt_name and having a list of those inside\n",
    "# TODO\n",
    "\n",
    "\n",
    "# 6th\n",
    "# no need to clean\n",
    "\n",
    "\n",
    "# 7th\n",
    "# change column types to save memory\n",
    "stats_df.filter(regex=\"_votes$\").astype(\"Int64\")\n",
    "stats_df[stats_df.filter(regex=\"_votes$\").columns] = (\n",
    "    stats_df.filter(regex=\"_votes$\").astype(\"Int64\")\n",
    ")\n",
    "\n",
    "\n",
    "# 8th\n",
    "# change column types to save memory\n",
    "ratings_df[\"is_rewatching\"] = ratings_df[\"is_rewatching\"].astype(\"Int8\")\n",
    "ratings_df[\"anime_id\"] = ratings_df[\"anime_id\"].astype(\"Int32\")\n",
    "ratings_df[\"score\"] = ratings_df[\"score\"].astype(\"Int8\")\n",
    "ratings_df[\"num_watched_episodes\"] = ratings_df[\"num_watched_episodes\"].astype(\"Int32\")\n",
    "# drop duplicates keeping the last entry (most recent)\n",
    "ratings_df.drop_duplicates(subset=['username', 'anime_id'], keep='last', inplace=True)\n",
    "# there's a username with NaN value, we'll keep it for now as it matches with the profiles_df\n",
    "# we only have 1 Nan value in \"username\" in the profiles_df so we can keep it for now\n",
    "# I could manage it this way: generate a deterministic placeholder, e.g.:\n",
    "# the maximum existing user_id + 1\n",
    "# or a specific labeled ID like \"unknown_user\"\n",
    "# THEN we can use this placeholder consistently across all datasets to maintain referential integrity.\n",
    "# THIS won't work though because user_id is missing in both datasets, how can we be sure that the Nan in profiles_df matches the Nan in ratings_df?\n",
    "# In this case it's easy because there's just one Nan in both datasets, but for future occasions we should do something else?\n",
    "# \n",
    "# profiles_df[\"user_id\"] = profiles_df[\"user_id\"].fillna(new_id)\n",
    "# ratings_df[\"user_id\"] = ratings_df[\"user_id\"].fillna(new_id)\n",
    "# TODO\n",
    "\n",
    "for chunk in pd.read_csv(\"datasets/ratings.csv\", chunksize=1_000_000):\n",
    "    chunk[\"is_rewatching\"] = chunk[\"is_rewatching\"].astype(\"Int8\")\n",
    "    chunk[\"anime_id\"] = chunk[\"anime_id\"].astype(\"Int32\")\n",
    "    chunk[\"score\"] = chunk[\"score\"].astype(\"Int8\")\n",
    "    chunk[\"num_watched_episodes\"] = chunk[\"num_watched_episodes\"].astype(\"Int32\")\n",
    "    \n",
    "    chunk.to_csv(\n",
    "        \"datasets/ratings_cleaned.csv\", mode=\"w\" if first_chunk else \"a\", \n",
    "        index=False, header=first_chunk,lineterminator=\"\\n\"\n",
    "    )\n",
    "    first_chunk = False\n",
    "\n",
    "ratings_cleaned_df = pd.read_csv(\"datasets/ratings_cleaned.csv\")\n",
    "ratings_cleaned_df.drop_duplicates(subset=[\"username\", \"anime_id\"], keep=\"last\", inplace=True)\n",
    "ratings_cleaned_df.to_csv(\"datasets/ratings_cleaned.csv\", index=False, lineterminator=\"\\n\")\n",
    "\n",
    "\n",
    "# 9th\n",
    "# change \"character_mal_id\" and \"favorites\" from float to int\n",
    "characters_df[[\"character_mal_id\", \"favorites\"]] = (\n",
    "    characters_df[[\"character_mal_id\", \"favorites\"]].astype(\"Int64\")\n",
    ")\n",
    "\n",
    "# we drop the two rows with all columns Nan\n",
    "characters_df.dropna(how='all', inplace=True)\n",
    "# we drop the duplicates because they have all same values \n",
    "characters_df.drop_duplicates(subset=['character_mal_id', 'url', 'name'], keep='first', inplace=True)\n",
    "\n",
    "\n",
    "# 10th\n",
    "# no need to clean\n",
    "\n",
    "\n",
    "# 11th\n",
    "person_voice_works_df.drop_duplicates(keep='first', inplace=True)\n",
    "\n",
    "\n",
    "# 12th\n",
    "# change column types from object to datetime\n",
    "profiles_df[[\"birthday\", \"joined\"]] = profiles_df[[\"birthday\", \"joined\"]].apply(\n",
    "    pd.to_datetime, errors=\"coerce\"\n",
    ")\n",
    "# setting birthdays outside a reasonable range to NaT\n",
    "profiles_df.loc[\n",
    "    profiles_df[\"birthday\"] > profiles_df[\"joined\"],\n",
    "    \"birthday\"\n",
    "] = pd.NaT\n",
    "# decided to remove the birthdays of people older than 100 at join\n",
    "mask_too_old = (profiles_df[\"joined\"] - profiles_df[\"birthday\"]).dt.days / 365.25 > 100\n",
    "profiles_df.loc[mask_too_old, \"birthday\"] = pd.NaT\n",
    "#I mean MongoDB, a female born in 1930-07-09, from Thailand and joined in 2017-07-09 makes perfect sense, doesn't it?\n",
    "\n",
    "# 13th\n",
    "# no need to clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d983d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b72608cba1bf6f29",
   "metadata": {},
   "source": [
    "### QUESTIONS:\n",
    "1. Can we just leave the commands to clean the dataset instead of leaving all the exploration commands such as .describe, dtypes and so on\n",
    "One file with data cleaning, showing the reasoning.\n",
    "ANOTHER file with Data Visualization.\n",
    "\n",
    "2. Ask for the maximum age in the 5th dataset\n",
    "Justify why you have changed the dates (e.g. change 2070 in 1970) or nan. Possiamo fare il cazzo che vogliamo\n",
    "\n",
    "3. Ask for the only NaN username in the \"ratings\" dataset: for the hci part?\n",
    "Think in terms of the future, take the latest id and sum 1, or progressive uknownuserssssssss (e.g. uknown1, unknown2 in caso poi riceviamo altri dati nel futuro)\n",
    "\n",
    "For TWEB we could just put a random (?) value in order not to lose the line and have non nan values in the database\n",
    "\n",
    "SE NON USIAMO COLONNE IN ASSOLUTO, per alcuna query o nel sito tweb\n",
    "Siccome non faremo mai query su questa cosa qui POSSIAMO TOGLIERE le colonne (e.g. kanji name lo possiamo togliere)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775cebe21ecfa0a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geopandas_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
