{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f0fc5d85df5ead7",
   "metadata": {},
   "source": [
    "### Beginning of the Assignment - exploration"
   ]
  },
  {
   "cell_type": "code",
   "id": "40caebbd49c3aae4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T17:52:36.557235Z",
     "start_time": "2025-12-15T17:52:36.553813Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "#pd.set_option('max_columns', 200)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "28bb707aefb5cf4",
   "metadata": {},
   "source": [
    "### PHIL DATA SET"
   ]
  },
  {
   "cell_type": "code",
   "id": "7e5b4d2ef82f5903",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T15:29:47.424479Z",
     "start_time": "2025-12-12T15:29:47.385479Z"
    }
   },
   "source": [
    "character_nicknames_df = pd.read_csv('datasets/PHIL DATA SET/character_nicknames.csv')"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "d69c4d7be6f9b0da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T15:29:50.753199Z",
     "start_time": "2025-12-12T15:29:50.349512Z"
    }
   },
   "source": [
    "details_df = pd.read_csv('datasets/PHIL DATA SET/details.csv')"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "47b151dad2401364",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T15:29:55.783610Z",
     "start_time": "2025-12-12T15:29:54.424922Z"
    }
   },
   "source": [
    "favs_df = pd.read_csv('datasets/PHIL DATA SET/favs.csv')\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "e121b1e7955869bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T15:29:58.376706Z",
     "start_time": "2025-12-12T15:29:58.098291Z"
    }
   },
   "source": [
    "person_details_df = pd.read_csv('datasets/PHIL DATA SET/person_details.csv')\n"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "db1624a96bf99276",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T15:29:59.342905Z",
     "start_time": "2025-12-12T15:29:59.316905Z"
    }
   },
   "source": [
    "person_alternate_names_df = pd.read_csv('datasets/PHIL DATA SET/person_alternate_names.csv')\n"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "698cb4f95d8af93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T15:30:04.166140Z",
     "start_time": "2025-12-12T15:30:03.825481Z"
    }
   },
   "source": "person_anime_works_df = pd.read_csv('datasets/PHIL DATA SET/person_anime_works.csv')\n",
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "3d3aa40b33b894e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T15:30:05.161751Z",
     "start_time": "2025-12-12T15:30:05.079030Z"
    }
   },
   "source": [
    "stats_df = pd.read_csv('datasets/PHIL DATA SET/stats.csv')\n"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "2e8396e73e4803de",
   "metadata": {},
   "source": [
    "### DENIS DATA SET"
   ]
  },
  {
   "cell_type": "code",
   "id": "3f1f790c14be4a88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T15:31:34.795537Z",
     "start_time": "2025-12-12T15:30:07.077041Z"
    }
   },
   "source": [
    "ratings_df = pd.read_csv('datasets/DENIS DATA SET/ratings.csv')"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "76032cb1462fb43b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T15:33:17.731522Z",
     "start_time": "2025-12-12T15:33:15.851275Z"
    }
   },
   "source": [
    "characters_df = pd.read_csv('datasets/DENIS DATA SET/characters.csv')"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "42ea38d0ed1db7ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T15:33:23.906707Z",
     "start_time": "2025-12-12T15:33:23.780769Z"
    }
   },
   "source": [
    "character_anime_works_df = pd.read_csv('datasets/DENIS DATA SET/character_anime_works.csv')"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "51cda4e6b11746a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T15:33:24.754306Z",
     "start_time": "2025-12-12T15:33:24.570871Z"
    }
   },
   "source": [
    "person_voice_works_df = pd.read_csv('datasets/DENIS DATA SET/person_voice_works.csv')"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "dbdb2c596fa23862",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T18:10:44.766961Z",
     "start_time": "2025-12-15T18:10:44.243869Z"
    }
   },
   "source": [
    "profiles_df = pd.read_csv('datasets/DENIS DATA SET/profiles.csv')"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "id": "a462170b7d3912c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T15:33:26.386872Z",
     "start_time": "2025-12-12T15:33:26.356003Z"
    }
   },
   "source": [
    "recommendations_df = pd.read_csv('datasets/DENIS DATA SET/recommendations.csv')"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "id": "9c4376ce",
   "metadata": {},
   "source": [
    "# GUIDELINES\n",
    "### BEFORE STARTING\n",
    "Use Conda in order to do the correct setup\n",
    "\n",
    "When we deliver the project, we need to tell the lecturer to run the jupyter notebook before running the TWEB part. How? Write it in the Report\n",
    "Give instructions in order to make it run properly\n",
    "\n",
    "#TODO to write on the report:\n",
    "Which are the cool things, problems, map names not allined so we had to normalize them and so on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcce46ca",
   "metadata": {},
   "source": [
    "Things to do for each dataset:\n",
    "1. Give it a look with .head and/or .tail\n",
    "2. .describe and check if all the numeric values make sense (e.g. year=300 makes no sense in our context)\n",
    "3. Check the format: objects to date if we need it. Check if all the dates are in the same format: us or eu\n",
    "    also check if there's any 29/02/2013. It doesn't exist right? Maybe this is too much lol\n",
    "4. Check for duplicates\n",
    "5. CHECK FOR CORRELATION: df.corr() (e.g. with longer duration, there are more actors)\n",
    "#TODO\n",
    "6. ADD or Remove columns?\n",
    "7. GROUPBY selects the elements and makes group out of it, combines the numeric fields of each specific group\n",
    "#TODO We could use it grouping for language and looking at how many anime are made in japan, stating it's the first country where the culture of doing (and watching) anime is SO big\n",
    "8. Aggregations: we can apply multiple different aggregated functions (e.g. for the first column you sum the data, for the second you do the average and so on)\n",
    "9. Transformations: apply operations and return results aligned with the original DF\n",
    "\n",
    "10. Removing NaNs is wrong in general because Pandas will skip it.\n",
    "We do it when? Is it safe to remove NaNs rows if EVERY field in the row is empty? I hope so lol\n",
    "BE CAREFUL if they are foreign keys: for example, if a person has a nan in the \"anime he worked in\" field, it shouldn't be dropped\n",
    "NEVER replace with invalid values (e.g. -1)\n",
    "IF we use df.dropna(subset=[\"name\"],inplace=True)\n",
    "the inplace means that the df itself is modified and will result in the one without the na. Without \"inplace=true\" you'll need to assign the result to another df (or the same) \n",
    "\n",
    "\n",
    "Proviamo i plot? df.plot()\n",
    "\n",
    "11. Check if data are consistent (e.g. normalizing names of countries and/or numeric fields, describing them and checking what they are)\n",
    "\n",
    "12. Normalize data types all in the same place (e.g. all the dates in the same cell)\n",
    "\n",
    "\n",
    "BONUS: NEVER USE LOOP FOR, NEVER DUPLICATE DATA (unless necessary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559aaa934a6028f2",
   "metadata": {},
   "source": [
    "##### First look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b52901143474e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T18:09:51.570461Z",
     "start_time": "2025-12-03T18:09:51.546798Z"
    }
   },
   "outputs": [],
   "source": [
    "character_nicknames_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdaecd75792d0bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T18:09:51.715797Z",
     "start_time": "2025-12-03T18:09:51.681944Z"
    }
   },
   "outputs": [],
   "source": [
    "character_nicknames_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f2346d047cbc84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T18:09:51.883862Z",
     "start_time": "2025-12-03T18:09:51.873486Z"
    }
   },
   "outputs": [],
   "source": [
    "character_nicknames_df.columns\n",
    "#will list all the columns. Not necessary here but kept as a pattern to follow with the following files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a166976d6165f4c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T18:09:51.944933Z",
     "start_time": "2025-12-03T18:09:51.935225Z"
    }
   },
   "outputs": [],
   "source": [
    "character_nicknames_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4eda35e9a19e60",
   "metadata": {},
   "source": [
    "### Data preparation (cleaning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71262e4b352d9474",
   "metadata": {},
   "source": [
    "##### On the first dataset we may need to check for duplicates.\n",
    "What does that mean? We have 102 rows that are duplicated over a 37080 rows dataset.\n",
    "Why is that? Are there some characters that have multiple nicknames, so they are repeated in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "id": "6f256644d9df068f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T15:33:40.422416Z",
     "start_time": "2025-12-12T15:33:40.299153Z"
    }
   },
   "source": [
    "character_nicknames_df.loc[character_nicknames_df.duplicated()]\n",
    "#by default will give us the second"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       character_mal_id        nickname\n",
       "328              276501            Koko\n",
       "329              276501         Cao Cao\n",
       "352              275883            King\n",
       "356              275835  Eldest Brother\n",
       "447              274437   The Half-Fool\n",
       "...                 ...             ...\n",
       "37051            281206          Apemon\n",
       "37052            281206          Apemon\n",
       "37053            281206          Apemon\n",
       "37054            281206          Apemon\n",
       "37068            281717         Colosso\n",
       "\n",
       "[102 rows x 2 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>character_mal_id</th>\n",
       "      <th>nickname</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>276501</td>\n",
       "      <td>Koko</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>276501</td>\n",
       "      <td>Cao Cao</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>275883</td>\n",
       "      <td>King</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>275835</td>\n",
       "      <td>Eldest Brother</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>274437</td>\n",
       "      <td>The Half-Fool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37051</th>\n",
       "      <td>281206</td>\n",
       "      <td>Apemon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37052</th>\n",
       "      <td>281206</td>\n",
       "      <td>Apemon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37053</th>\n",
       "      <td>281206</td>\n",
       "      <td>Apemon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37054</th>\n",
       "      <td>281206</td>\n",
       "      <td>Apemon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37068</th>\n",
       "      <td>281717</td>\n",
       "      <td>Colosso</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102 rows × 2 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "id": "d0ba43973017cba6",
   "metadata": {},
   "source": [
    "Mhh they're somehow different so yeah, the same character could have different nicknames.\n",
    "We want to drop though the ones that are exactly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344037bd6d62638b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T18:09:54.980944Z",
     "start_time": "2025-12-03T18:09:54.950703Z"
    }
   },
   "outputs": [],
   "source": [
    "#this way we drop the duplicates on the first dataset\n",
    "\n",
    "# OLD VERSION \n",
    "# character_nicknames_df = character_nicknames_df.loc[~character_nicknames_df.duplicated()].reset_index(drop=True).copy()\n",
    "\n",
    "#we don't need to use a subset here because there are just 2 columns\n",
    "\n",
    "#why don't we just use drop_duplicates()?\n",
    "character_nicknames_df.drop_duplicates(keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ca6149ba118158",
   "metadata": {},
   "source": [
    "Let's check for nan values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6168da3a85323cf0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T18:09:59.373588Z",
     "start_time": "2025-12-03T18:09:59.359680Z"
    }
   },
   "outputs": [],
   "source": [
    "character_nicknames_df[character_nicknames_df.isna().any(axis=1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bfce02f7b6df3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T18:10:00.080752Z",
     "start_time": "2025-12-03T18:10:00.067960Z"
    }
   },
   "outputs": [],
   "source": [
    "#cleaning the df from nan values\n",
    "character_nicknames_df.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ecacd990a27e5b",
   "metadata": {},
   "source": [
    "## SECOND DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec30ca445b47f67",
   "metadata": {},
   "source": [
    "##### On the second dataset we may need to check for missing values and/or inconsistent values, since there are no duplicates"
   ]
  },
  {
   "cell_type": "code",
   "id": "2d9e440942e0bc9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T15:51:33.992685Z",
     "start_time": "2025-12-12T15:51:33.937995Z"
    }
   },
   "source": [
    "details_df\n",
    "#anime details\n",
    "#japanes title could be dropped?\n",
    "#members stand for how many users have this anime added to their list.\n",
    "#explicit_genres is empty so can be removed\n",
    "#licensor and streaming are mostly empty. Do we care?"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       mal_id                                              title  \\\n",
       "0       59356                                           -Socket-   \n",
       "1       56036                                             ......   \n",
       "2        2928                               .hack//G.U. Returner   \n",
       "3        3269                                .hack//G.U. Trilogy   \n",
       "4        4469                   .hack//G.U. Trilogy: Parody Mode   \n",
       "...       ...                                                ...   \n",
       "28950   59421  Zutaboro Reijou wa Ane no Moto Konyakusha ni D...   \n",
       "28951   31245  Zutto Mae kara Suki deshita. Kokuhaku Jikkou I...   \n",
       "28952   36305  Zutto Mae kara Suki deshita. Kokuhaku Jikkou I...   \n",
       "28953   34895                                   Zutto Suki Datta   \n",
       "28954   56606                                    Zutto Tomodachi   \n",
       "\n",
       "                         title_japanese  \\\n",
       "0                              -socket-   \n",
       "1                                ......   \n",
       "2                  .HACK//G.U. RETURNER   \n",
       "3                   .hack//G.U. Trilogy   \n",
       "4                   .hack//G.U. Trilogy   \n",
       "...                                 ...   \n",
       "28950               ずたぼろ令嬢は姉の元婚約者に溺愛される   \n",
       "28951             ずっと前から好きでした。～告白実行委員会～   \n",
       "28952  ずっと前から好きでした。～告白実行委員会～ 「金曜日のおはよう」   \n",
       "28953                          ずっと好きだった   \n",
       "28954                             ずっと友達   \n",
       "\n",
       "                                                     url  \\\n",
       "0           https://myanimelist.net/anime/59356/-Socket-   \n",
       "1                  https://myanimelist.net/anime/56036/-   \n",
       "2      https://myanimelist.net/anime/2928/hack__GU_Re...   \n",
       "3      https://myanimelist.net/anime/3269/hack__GU_Tr...   \n",
       "4      https://myanimelist.net/anime/4469/hack__GU_Tr...   \n",
       "...                                                  ...   \n",
       "28950  https://myanimelist.net/anime/59421/Zutaboro_R...   \n",
       "28951  https://myanimelist.net/anime/31245/Zutto_Mae_...   \n",
       "28952  https://myanimelist.net/anime/36305/Zutto_Mae_...   \n",
       "28953  https://myanimelist.net/anime/34895/Zutto_Suki...   \n",
       "28954  https://myanimelist.net/anime/56606/Zutto_Tomo...   \n",
       "\n",
       "                                               image_url     type  \\\n",
       "0      https://cdn.myanimelist.net/images/anime/1043/...    Movie   \n",
       "1      https://cdn.myanimelist.net/images/anime/1057/...    Music   \n",
       "2      https://cdn.myanimelist.net/images/anime/1798/...      OVA   \n",
       "3      https://cdn.myanimelist.net/images/anime/1566/...    Movie   \n",
       "4      https://cdn.myanimelist.net/images/anime/10/86...  Special   \n",
       "...                                                  ...      ...   \n",
       "28950  https://cdn.myanimelist.net/images/anime/1518/...       TV   \n",
       "28951  https://cdn.myanimelist.net/images/anime/3/821...    Movie   \n",
       "28952  https://cdn.myanimelist.net/images/anime/6/883...  Special   \n",
       "28953  https://cdn.myanimelist.net/images/anime/1498/...      OVA   \n",
       "28954  https://cdn.myanimelist.net/images/anime/1462/...    Music   \n",
       "\n",
       "                status  score  scored_by                start_date  ...  \\\n",
       "0      Finished Airing    NaN        NaN 2010-01-01 00:00:00+00:00  ...   \n",
       "1      Finished Airing   6.53      503.0 2023-06-11 00:00:00+00:00  ...   \n",
       "2      Finished Airing   6.65     9745.0 2007-01-18 00:00:00+00:00  ...   \n",
       "3      Finished Airing   7.06    15373.0 2007-12-22 00:00:00+00:00  ...   \n",
       "4      Finished Airing   6.35     4317.0 2008-03-25 00:00:00+00:00  ...   \n",
       "...                ...    ...        ...                       ...  ...   \n",
       "28950  Finished Airing   7.37    15624.0 2025-07-05 00:00:00+00:00  ...   \n",
       "28951  Finished Airing   7.20   104106.0 2016-04-23 00:00:00+00:00  ...   \n",
       "28952  Finished Airing   7.17    10038.0 2016-10-26 00:00:00+00:00  ...   \n",
       "28953  Finished Airing   5.68     1887.0 2017-04-21 00:00:00+00:00  ...   \n",
       "28954  Finished Airing    NaN        NaN 1996-06-01 00:00:00+00:00  ...   \n",
       "\n",
       "      demographics       source                     rating  episodes  season  \\\n",
       "0               []     Original               G - All Ages       1.0     NaN   \n",
       "1               []     Original  PG-13 - Teens 13 or older       1.0     NaN   \n",
       "2               []         Game  PG-13 - Teens 13 or older       1.0     NaN   \n",
       "3               []         Game  PG-13 - Teens 13 or older       1.0     NaN   \n",
       "4               []         Game  PG-13 - Teens 13 or older       1.0     NaN   \n",
       "...            ...          ...                        ...       ...     ...   \n",
       "28950    ['Josei']  Light novel  PG-13 - Teens 13 or older      12.0  summer   \n",
       "28951           []        Music  PG-13 - Teens 13 or older       1.0     NaN   \n",
       "28952           []        Music              PG - Children       1.0     NaN   \n",
       "28953           []        Manga                Rx - Hentai       2.0     NaN   \n",
       "28954     ['Kids']     Original               G - All Ages       1.0     NaN   \n",
       "\n",
       "         year                                          producers  \\\n",
       "0         NaN                        ['Nagoya Zokei University']   \n",
       "1         NaN                                                 []   \n",
       "2         NaN                 ['Bandai Visual', 'CyberConnect2']   \n",
       "3         NaN                                  ['Bandai Visual']   \n",
       "4         NaN                                  ['Bandai Visual']   \n",
       "...       ...                                                ...   \n",
       "28950  2025.0  ['Studio Pierrot', 'Mainichi Broadcasting Syst...   \n",
       "28951     NaN  ['Aniplex', 'Dentsu', 'Kadokawa Shoten', 'Movi...   \n",
       "28952     NaN                                        ['Aniplex']   \n",
       "28953     NaN                         ['Queen Bee', 'Mediabank']   \n",
       "28954     NaN                                            ['NHK']   \n",
       "\n",
       "      explicit_genres                               licensors  \\\n",
       "0                  []                                      []   \n",
       "1                  []                                      []   \n",
       "2                  []                                      []   \n",
       "3                  []  ['Funimation', 'Bandai Entertainment']   \n",
       "4                  []                                      []   \n",
       "...               ...                                     ...   \n",
       "28950              []                                      []   \n",
       "28951              []                  ['Aniplex of America']   \n",
       "28952              []                                      []   \n",
       "28953              []                                      []   \n",
       "28954              []                                      []   \n",
       "\n",
       "                                               streaming  \n",
       "0                                                     []  \n",
       "1                                                     []  \n",
       "2                                                     []  \n",
       "3                                                     []  \n",
       "4                                                     []  \n",
       "...                                                  ...  \n",
       "28950  ['Crunchyroll', 'Aniplus TV', 'Bahamut Anime C...  \n",
       "28951                                                 []  \n",
       "28952                                                 []  \n",
       "28953                                                 []  \n",
       "28954                                                 []  \n",
       "\n",
       "[28955 rows x 29 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mal_id</th>\n",
       "      <th>title</th>\n",
       "      <th>title_japanese</th>\n",
       "      <th>url</th>\n",
       "      <th>image_url</th>\n",
       "      <th>type</th>\n",
       "      <th>status</th>\n",
       "      <th>score</th>\n",
       "      <th>scored_by</th>\n",
       "      <th>start_date</th>\n",
       "      <th>...</th>\n",
       "      <th>demographics</th>\n",
       "      <th>source</th>\n",
       "      <th>rating</th>\n",
       "      <th>episodes</th>\n",
       "      <th>season</th>\n",
       "      <th>year</th>\n",
       "      <th>producers</th>\n",
       "      <th>explicit_genres</th>\n",
       "      <th>licensors</th>\n",
       "      <th>streaming</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59356</td>\n",
       "      <td>-Socket-</td>\n",
       "      <td>-socket-</td>\n",
       "      <td>https://myanimelist.net/anime/59356/-Socket-</td>\n",
       "      <td>https://cdn.myanimelist.net/images/anime/1043/...</td>\n",
       "      <td>Movie</td>\n",
       "      <td>Finished Airing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2010-01-01 00:00:00+00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Original</td>\n",
       "      <td>G - All Ages</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Nagoya Zokei University']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56036</td>\n",
       "      <td>......</td>\n",
       "      <td>......</td>\n",
       "      <td>https://myanimelist.net/anime/56036/-</td>\n",
       "      <td>https://cdn.myanimelist.net/images/anime/1057/...</td>\n",
       "      <td>Music</td>\n",
       "      <td>Finished Airing</td>\n",
       "      <td>6.53</td>\n",
       "      <td>503.0</td>\n",
       "      <td>2023-06-11 00:00:00+00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Original</td>\n",
       "      <td>PG-13 - Teens 13 or older</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2928</td>\n",
       "      <td>.hack//G.U. Returner</td>\n",
       "      <td>.HACK//G.U. RETURNER</td>\n",
       "      <td>https://myanimelist.net/anime/2928/hack__GU_Re...</td>\n",
       "      <td>https://cdn.myanimelist.net/images/anime/1798/...</td>\n",
       "      <td>OVA</td>\n",
       "      <td>Finished Airing</td>\n",
       "      <td>6.65</td>\n",
       "      <td>9745.0</td>\n",
       "      <td>2007-01-18 00:00:00+00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Game</td>\n",
       "      <td>PG-13 - Teens 13 or older</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Bandai Visual', 'CyberConnect2']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3269</td>\n",
       "      <td>.hack//G.U. Trilogy</td>\n",
       "      <td>.hack//G.U. Trilogy</td>\n",
       "      <td>https://myanimelist.net/anime/3269/hack__GU_Tr...</td>\n",
       "      <td>https://cdn.myanimelist.net/images/anime/1566/...</td>\n",
       "      <td>Movie</td>\n",
       "      <td>Finished Airing</td>\n",
       "      <td>7.06</td>\n",
       "      <td>15373.0</td>\n",
       "      <td>2007-12-22 00:00:00+00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Game</td>\n",
       "      <td>PG-13 - Teens 13 or older</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Bandai Visual']</td>\n",
       "      <td>[]</td>\n",
       "      <td>['Funimation', 'Bandai Entertainment']</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4469</td>\n",
       "      <td>.hack//G.U. Trilogy: Parody Mode</td>\n",
       "      <td>.hack//G.U. Trilogy</td>\n",
       "      <td>https://myanimelist.net/anime/4469/hack__GU_Tr...</td>\n",
       "      <td>https://cdn.myanimelist.net/images/anime/10/86...</td>\n",
       "      <td>Special</td>\n",
       "      <td>Finished Airing</td>\n",
       "      <td>6.35</td>\n",
       "      <td>4317.0</td>\n",
       "      <td>2008-03-25 00:00:00+00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Game</td>\n",
       "      <td>PG-13 - Teens 13 or older</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Bandai Visual']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28950</th>\n",
       "      <td>59421</td>\n",
       "      <td>Zutaboro Reijou wa Ane no Moto Konyakusha ni D...</td>\n",
       "      <td>ずたぼろ令嬢は姉の元婚約者に溺愛される</td>\n",
       "      <td>https://myanimelist.net/anime/59421/Zutaboro_R...</td>\n",
       "      <td>https://cdn.myanimelist.net/images/anime/1518/...</td>\n",
       "      <td>TV</td>\n",
       "      <td>Finished Airing</td>\n",
       "      <td>7.37</td>\n",
       "      <td>15624.0</td>\n",
       "      <td>2025-07-05 00:00:00+00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>['Josei']</td>\n",
       "      <td>Light novel</td>\n",
       "      <td>PG-13 - Teens 13 or older</td>\n",
       "      <td>12.0</td>\n",
       "      <td>summer</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>['Studio Pierrot', 'Mainichi Broadcasting Syst...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>['Crunchyroll', 'Aniplus TV', 'Bahamut Anime C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28951</th>\n",
       "      <td>31245</td>\n",
       "      <td>Zutto Mae kara Suki deshita. Kokuhaku Jikkou I...</td>\n",
       "      <td>ずっと前から好きでした。～告白実行委員会～</td>\n",
       "      <td>https://myanimelist.net/anime/31245/Zutto_Mae_...</td>\n",
       "      <td>https://cdn.myanimelist.net/images/anime/3/821...</td>\n",
       "      <td>Movie</td>\n",
       "      <td>Finished Airing</td>\n",
       "      <td>7.20</td>\n",
       "      <td>104106.0</td>\n",
       "      <td>2016-04-23 00:00:00+00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Music</td>\n",
       "      <td>PG-13 - Teens 13 or older</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Aniplex', 'Dentsu', 'Kadokawa Shoten', 'Movi...</td>\n",
       "      <td>[]</td>\n",
       "      <td>['Aniplex of America']</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28952</th>\n",
       "      <td>36305</td>\n",
       "      <td>Zutto Mae kara Suki deshita. Kokuhaku Jikkou I...</td>\n",
       "      <td>ずっと前から好きでした。～告白実行委員会～ 「金曜日のおはよう」</td>\n",
       "      <td>https://myanimelist.net/anime/36305/Zutto_Mae_...</td>\n",
       "      <td>https://cdn.myanimelist.net/images/anime/6/883...</td>\n",
       "      <td>Special</td>\n",
       "      <td>Finished Airing</td>\n",
       "      <td>7.17</td>\n",
       "      <td>10038.0</td>\n",
       "      <td>2016-10-26 00:00:00+00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Music</td>\n",
       "      <td>PG - Children</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Aniplex']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28953</th>\n",
       "      <td>34895</td>\n",
       "      <td>Zutto Suki Datta</td>\n",
       "      <td>ずっと好きだった</td>\n",
       "      <td>https://myanimelist.net/anime/34895/Zutto_Suki...</td>\n",
       "      <td>https://cdn.myanimelist.net/images/anime/1498/...</td>\n",
       "      <td>OVA</td>\n",
       "      <td>Finished Airing</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1887.0</td>\n",
       "      <td>2017-04-21 00:00:00+00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Manga</td>\n",
       "      <td>Rx - Hentai</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Queen Bee', 'Mediabank']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28954</th>\n",
       "      <td>56606</td>\n",
       "      <td>Zutto Tomodachi</td>\n",
       "      <td>ずっと友達</td>\n",
       "      <td>https://myanimelist.net/anime/56606/Zutto_Tomo...</td>\n",
       "      <td>https://cdn.myanimelist.net/images/anime/1462/...</td>\n",
       "      <td>Music</td>\n",
       "      <td>Finished Airing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1996-06-01 00:00:00+00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>['Kids']</td>\n",
       "      <td>Original</td>\n",
       "      <td>G - All Ages</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['NHK']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28955 rows × 29 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# we want to see what are \"type\"\n",
    "details_df['type'].value_counts()"
   ],
   "id": "58fd3fd28c2741f2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "details_df.loc[details_df.duplicated(subset=['url'], keep='first')]",
   "id": "b138de4f13369292"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "details_df.query('year>2025')",
   "id": "13d31f89c6105ccd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "details_df.describe()",
   "id": "8d5895d17ad81645"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "details_df[['start_date','season']].query(\"season.notna()\")\n",
    "#season can be removed? Do we care about the season? We can \"calculate\" it from the \"start_date\" field"
   ],
   "id": "3968075d4de60cd4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "details_df.query(\"episodes > 2500\")",
   "id": "c6aa49e2052e976c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "details_df[[\"start_date\", \"end_date\"]] = details_df[[\"start_date\", \"end_date\"]].apply(\n",
    "    pd.to_datetime, errors=\"coerce\"\n",
    ")"
   ],
   "id": "385d854ee6b9da6f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "details_df[[\"scored_by\", \"rank\", \"episodes\", \"year\"]] = (\n",
    "    details_df[[\"scored_by\", \"rank\", \"episodes\", \"year\"]].astype(\"Int64\")\n",
    ")\n"
   ],
   "id": "9c760cdab33dc03e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "details_df.dtypes\n",
    "#scored_by, rank, episodes, year can be an int instead of a float\n",
    "#start and end dates are not objects but dates\n",
    "#do we need to swap the empty [] with Nan or not? WE should in order to be able to use the .isna() method and other pandas methods\n"
   ],
   "id": "fd8ab91e14a99bf4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### THIRD DATASET",
   "id": "8f6bcd8d159d2711"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "favs_df",
   "id": "6482f5884fe7415d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# we want to see what are \"fav_type\"\n",
    "favs_df['fav_type'].value_counts()"
   ],
   "id": "38cb47295a56f1b3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "favs_df.isna().sum()\n",
   "id": "c60745a14aa28b44"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "favs_df.dtypes",
   "id": "51e310ab028ae5bc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "favs_df.duplicated().sum()\n",
    "#there are no duplicates\n"
   ],
   "id": "847b46866e24884a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### FOURTH DATASET",
   "id": "7e7a4654641134d5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "person_alternate_names_df",
   "id": "92462999a9ad06ec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "person_alternate_names_df.dtypes",
   "id": "9bed1ff9faa6c1da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "person_alternate_names_df.isna().sum()\n",
    "person_alternate_names_df[person_alternate_names_df.isna().any(axis=1)]\n"
   ],
   "id": "3c4fbdfaff7fc108"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "person_alternate_names_df.dropna(inplace=True)\n",
    "\n"
   ],
   "id": "a29d9f6f81c3461f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# person_alternate_names_df.loc[person_alternate_names_df['person_mal_id'].duplicated()]\n",
    "person_alternate_names_df[person_alternate_names_df.duplicated(subset=['person_mal_id','alt_name'], keep=False)].sort_values(['person_mal_id','alt_name'])\n"
   ],
   "id": "43d51a9e80efc1a7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "person_alternate_names_df.drop_duplicates(keep='first', inplace=True)",
   "id": "50b8a9ad9f84a4dc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### FIFTH DATASET",
   "id": "6b4b9da82ba09c53"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "person_details_df",
   "id": "43304e3b2f8067d1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "person_details_df.loc[person_details_df['person_mal_id'].duplicated()]\n",
    "#we found that the duplicates differ for the \"relevant_location\" field, which has no interest for us so we drop the duplicates"
   ],
   "id": "1efd1309a834162b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "person_details_df.drop_duplicates(subset=['person_mal_id', 'url', 'name'], keep='first', inplace=True)\n",
   "id": "e928415c99210775"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "person_details_df.dtypes\n",
    "#we need to change birthday from object to data"
   ],
   "id": "f2d92e7520dfb544"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "person_details_df[\"birthday\"] = pd.to_datetime(person_details_df[\"birthday\"], errors='coerce')",
   "id": "994211cb9f67f0ab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "person_details_df[\"birthday\"].min(), person_details_df[\"birthday\"].max()\n",
    "#makes sense because they're just composers of used music in anime"
   ],
   "id": "f5b630223a2ab165"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "person_details_df.isna().sum()\n",
    "#we have to check the nan values"
   ],
   "id": "9bf1e3ccd4f377a4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "person_details_df[person_details_df[\"name\"].isna()]\n",
   "id": "777d73883a4885ac"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We can join the two tables person_details_df and person_alternate_names_df having the keys that match.\n",
    "Putting the alternate names in a new column called alt_name and having a list of those inside"
   ],
   "id": "c96597e9889f1c36"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### SIXTH DATASET",
   "id": "646233103f95bf88"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "person_anime_works_df",
   "id": "ec8a185495c0fac5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "person_anime_works_df.dtypes\n",
    "#the types are correct"
   ],
   "id": "f0985945f44ea99a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "person_anime_works_df.isna().sum()\n",
    "#There's no nan value"
   ],
   "id": "898138dd7fe4dc58"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### SEVENTH DATASET",
   "id": "d9ad2cfe2be526dd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "stats_df.filter(regex=\"_votes$\").astype(\"Int64\")\n",
    "stats_df[stats_df.filter(regex=\"_votes$\").columns] = (\n",
    "    stats_df.filter(regex=\"_votes$\").astype(\"Int64\")\n",
    ")"
   ],
   "id": "faf7e3a021ac0845"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "stats_df.isna().sum()\n",
    "#there are 430 series without any votes"
   ],
   "id": "f39a3de40a42ce78"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### EIGHTH DATASET ",
   "id": "3f8375f8980f805a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "ratings_df",
   "id": "81148221c3ef57cb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "ratings_df.columns()",
   "id": "e8cfecd02cabce78"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# we have to understand the sense of \"num_watched_episodes\" and the link with \"is_rewatching\"\n",
    "ratings_df.query('is_rewatching == 1')"
   ],
   "id": "45f6a516eb227ac1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "ratings_df.dtypes",
   "id": "3befb102a25b244d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# change \"is_rewatching\" from float to Int8, to save memory\n",
    "ratings_df[\"is_rewatching\"] = ratings_df[\"is_rewatching\"].astype(\"Int8\") "
   ],
   "id": "7f5ce74ce49118f9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "ratings_df[[\"anime_id\",\"score\",\"num_watched_episodes\"]].agg([\"min\", \"max\"])\n",
   "id": "86b120acc6ab51b6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# to save up some memory, we can change \"anime_id\" from Int64 to Int32 because there are no anime with id > 2,147,483,647\n",
    "ratings_df[\"anime_id\"] = ratings_df[\"anime_id\"].astype(\"Int32\")"
   ],
   "id": "d0ef0a28383f6b37"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "ratings_df[ratings_df.duplicated(subset=['username','anime_id'], keep=False)].sort_values(['username','anime_id'])",
   "id": "a16e5c89288a2f2d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# usually we should drop all the occurrence of a duplicate and keep the first\n",
    "# in this case though, it looks like the latest occurence is the most updated one, contaning more info than the first one, so we drop the first one\n",
    "ratings_df.drop_duplicates(subset=['username', 'anime_id'], keep='last', inplace=True)"
   ],
   "id": "33e55904174dc326"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We had just 6 duplicates having the same username and anime_id",
   "id": "dce1bf5a74967db2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# we check for Nan values.\n",
    "#TODO\n",
    "# if it is necessary check if the num_watched_episodes is greater than number of episodes of anime, we can remove the Nan values and put one or zero. \n",
    "ratings_df.isna().sum()"
   ],
   "id": "69cb137cda84d449"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# drop \"username\" with Nan values?\n",
    "#TODO\n",
    "ratings_df[ratings_df['username'].isna()]"
   ],
   "id": "d32609afbb47253b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Check this username that there is in the profiles_df",
   "id": "898fc17a38038251"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### NINTH DATASET",
   "id": "2fd009e3cb2f1d55"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "characters_df",
   "id": "f132c9c5b439865b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# check types of dataset columns\n",
    "characters_df.dtypes"
   ],
   "id": "edf77859b4c1fea2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# change \"character_mal_id\" and \"favorites\" from float to int\n",
    "characters_df[\"character_mal_id\"] = characters_df[\"character_mal_id\"].astype(\"Int64\")\n",
    "characters_df[\"favorites\"] = characters_df[\"favorites\"].astype(\"Int64\")"
   ],
   "id": "5b90d91c87325e3f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "characters_df.describe()",
   "id": "262b59d9e7ab712f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# we have only 2 rows where all columns are Nan, the rows with Nan values in \"name_kanji\" and \"about\" we shouldn't drop because they have other values that are important.\n",
    "characters_df.isna().sum()"
   ],
   "id": "d51ea147dbb548de"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# here we want to check if the Nan values are concentrate in only two rows\n",
    "characters_df[characters_df['character_mal_id'].isna()]"
   ],
   "id": "c83998fbda1510ad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Apart \"name_kanji\" and \"about\" the others Nan values are concentrate in two rows so we drop the two rows with all columns Nan\n",
    "characters_df.dropna(how='all', inplace=True)\n"
   ],
   "id": "ec922906053b3b4b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# we want to see all duplicates to understand if we have to drop or not\n",
    "characters_df.loc[characters_df.duplicated(subset=['character_mal_id', 'url', 'name'], keep=False)]"
   ],
   "id": "38e740ccc88a1dca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# we drop the duplicates because they have all same values \n",
    "characters_df.drop_duplicates(subset=['character_mal_id', 'url', 'name'], keep='first', inplace=True)"
   ],
   "id": "2e396358a0b99436"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### TENTH DATASET",
   "id": "ee5c4ec82f24619"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# role of character anime\n",
    "character_anime_works_df"
   ],
   "id": "5f9a7e6f14d4576d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# check types of columns\n",
    "character_anime_works_df.dtypes"
   ],
   "id": "8fa5d2f0fdf5098b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# check the number of Nan value\n",
    "character_anime_works_df.isna().sum()"
   ],
   "id": "8eb229ef9b49bc61"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# check the number of duplicates\n",
    "character_anime_works_df.loc[character_anime_works_df.duplicated(subset=['anime_mal_id', 'character_mal_id'])]"
   ],
   "id": "b5fc95f21626ff18"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "There is no need to clean this dataset ",
   "id": "24326514f39c0ae5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### ELEVENTH DATASET",
   "id": "7f8ade7b84a24dc7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "person_voice_works_df",
   "id": "6bdc70d72f90897"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "person_voice_works_df['language'].value_counts()",
   "id": "4bb8ca8a047c4d1f"
  },
  {
   "cell_type": "code",
   "id": "c809d5c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T12:42:38.087189500Z",
     "start_time": "2025-12-12T12:41:09.172172Z"
    }
   },
   "source": "person_voice_works_df['language'].sum()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89551dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I want to group by language and see the different languages available in the dataset.\n",
    "person_voice_works_df.groupby('language').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205eaacb00fff92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_voice_works_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63783dc49ac28a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_voice_works_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9839d0c9d8d719bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the duplicates are in all columns\n",
    "person_voice_works_df.loc[person_voice_works_df.duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc6b8c4e4eb2778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the duplicates because they have all same values\n",
    "person_voice_works_df.drop_duplicates(keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a409aef3a0a83a9",
   "metadata": {},
   "source": [
    "### TWELFTH DATASET"
   ]
  },
  {
   "cell_type": "code",
   "id": "842fdcde8d97a4f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T18:12:25.801898Z",
     "start_time": "2025-12-15T18:12:25.788989Z"
    }
   },
   "source": [
    "# Should we delete the last five columns?\n",
    "# TODO\n",
    "profiles_df"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "           username  gender      birthday       location        joined  \\\n",
       "0         ishikawas     NaN           NaN    South Korea           NaN   \n",
       "1              CKK2     NaN           NaN  United States   Dec 1, 2018   \n",
       "2       --------788  Female           NaN         Mexico   Oct 4, 2022   \n",
       "3        potatoaris     NaN           NaN          Spain   Oct 2, 2018   \n",
       "4         Rinrintan     NaN           NaN          Japan  May 12, 2019   \n",
       "...             ...     ...           ...            ...           ...   \n",
       "337150  ariyanroy04     NaN           NaN  United States  Mar 30, 2021   \n",
       "337151   ariyanvk18    Male           NaN         Turkey  Jan 24, 2022   \n",
       "337152     ariyoskz     NaN           NaN        Germany   Jul 7, 2022   \n",
       "337153    arizima23  Female  Jun 23, 1998          Spain  Mar 15, 2019   \n",
       "337154      arizkim     NaN           NaN   South Africa   Dec 7, 2020   \n",
       "\n",
       "       watching completed on_hold dropped plan_to_watch  \n",
       "0           NaN       NaN     NaN     NaN           NaN  \n",
       "1             3       182      15       0           405  \n",
       "2             1        64       0       0             1  \n",
       "3             5         1       0       0             4  \n",
       "4            20       311      40      16            34  \n",
       "...         ...       ...     ...     ...           ...  \n",
       "337150       35       130       0       0           489  \n",
       "337151        7       101       1      12            21  \n",
       "337152        1       155       2      11            15  \n",
       "337153       15        64       0       4             4  \n",
       "337154        7        38       1       0            33  \n",
       "\n",
       "[337155 rows x 10 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>gender</th>\n",
       "      <th>birthday</th>\n",
       "      <th>location</th>\n",
       "      <th>joined</th>\n",
       "      <th>watching</th>\n",
       "      <th>completed</th>\n",
       "      <th>on_hold</th>\n",
       "      <th>dropped</th>\n",
       "      <th>plan_to_watch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ishikawas</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>South Korea</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CKK2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States</td>\n",
       "      <td>Dec 1, 2018</td>\n",
       "      <td>3</td>\n",
       "      <td>182</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>--------788</td>\n",
       "      <td>Female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>Oct 4, 2022</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>potatoaris</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Oct 2, 2018</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rinrintan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Japan</td>\n",
       "      <td>May 12, 2019</td>\n",
       "      <td>20</td>\n",
       "      <td>311</td>\n",
       "      <td>40</td>\n",
       "      <td>16</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337150</th>\n",
       "      <td>ariyanroy04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States</td>\n",
       "      <td>Mar 30, 2021</td>\n",
       "      <td>35</td>\n",
       "      <td>130</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337151</th>\n",
       "      <td>ariyanvk18</td>\n",
       "      <td>Male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Turkey</td>\n",
       "      <td>Jan 24, 2022</td>\n",
       "      <td>7</td>\n",
       "      <td>101</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337152</th>\n",
       "      <td>ariyoskz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Jul 7, 2022</td>\n",
       "      <td>1</td>\n",
       "      <td>155</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337153</th>\n",
       "      <td>arizima23</td>\n",
       "      <td>Female</td>\n",
       "      <td>Jun 23, 1998</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Mar 15, 2019</td>\n",
       "      <td>15</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337154</th>\n",
       "      <td>arizkim</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>South Africa</td>\n",
       "      <td>Dec 7, 2020</td>\n",
       "      <td>7</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>337155 rows × 10 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6850cf22694f24bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the types are right for each field\n",
    "profiles_df.dtypes"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T18:12:48.446264Z",
     "start_time": "2025-12-15T18:12:48.438124Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# trying things with the date\n",
    "# BEFORE running the .to_datetime command, ishiyama_yumi has Jul 24 bday\n",
    "profiles_df.loc[44]"
   ],
   "id": "30f1434b0fc3b4b9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "username         ishiyama_yumi\n",
       "gender                  Female\n",
       "birthday                Jul 24\n",
       "location              Thailand\n",
       "joined            Dec 16, 2020\n",
       "watching                    10\n",
       "completed                  308\n",
       "on_hold                     16\n",
       "dropped                     56\n",
       "plan_to_watch              143\n",
       "Name: 44, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "id": "23e0cfca8b069e17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T18:13:34.540997Z",
     "start_time": "2025-12-15T18:13:33.626977Z"
    }
   },
   "source": [
    "# change types of columns \"birthday\" and \"joined\" from object to date and the others columns that they should be int\n",
    "profiles_df[\"birthday\"] = pd.to_datetime(profiles_df[\"birthday\"], errors='coerce')\n",
    "profiles_df[\"joined\"] = pd.to_datetime(profiles_df[\"joined\"], errors='coerce')"
   ],
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T18:13:44.448038Z",
     "start_time": "2025-12-15T18:13:44.439862Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# AFTER running the .to_datetime command, ishiyama_yumi has NaT birthday\n",
    "profiles_df.loc[44]"
   ],
   "id": "4a0fc56b5221543",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "username               ishiyama_yumi\n",
       "gender                        Female\n",
       "birthday                         NaT\n",
       "location                    Thailand\n",
       "joined           2020-12-16 00:00:00\n",
       "watching                          10\n",
       "completed                        308\n",
       "on_hold                           16\n",
       "dropped                           56\n",
       "plan_to_watch                    143\n",
       "Name: 44, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "id": "e04e07a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T17:54:53.832104Z",
     "start_time": "2025-12-15T17:54:53.815141Z"
    }
   },
   "source": [
    "profiles_df[\"birthday\"].min(), profiles_df[\"birthday\"].max()\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Timestamp('1800-01-01 00:00:00'), Timestamp('2100-12-31 00:00:00'))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "0ed71d85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T18:12:36.808122Z",
     "start_time": "2025-12-15T18:12:36.752673Z"
    }
   },
   "source": [
    "weird_birthdays = profiles_df[\n",
    "    (profiles_df[\"birthday\"] < \"1900-01-01\") |\n",
    "    (profiles_df[\"birthday\"] > \"2025-12-31\")\n",
    "]\n",
    "\n",
    "weird_birthdays\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "           username  gender      birthday   location        joined watching  \\\n",
       "15           RinsAl    Male   Jul 7, 1989      Japan  Oct 19, 2009       16   \n",
       "19         ishimori  Female  Apr 17, 2000     France   Dec 7, 2018        1   \n",
       "23       Karinnn___     NaN  Feb 28, 2001  Australia   Aug 7, 2018        2   \n",
       "25            Rins_    Male  Jul 15, 1988      Japan   Jun 6, 2013        5   \n",
       "29      FollowValen    Male   Jul 8, 1996      Spain   Oct 7, 2018        9   \n",
       "...             ...     ...           ...        ...           ...      ...   \n",
       "337137    arisvgawa  Female   Mar 3, 2001      Japan   Oct 2, 2024        1   \n",
       "337138      aritaka     NaN         Aug 4    Germany  Apr 24, 2012       18   \n",
       "337143     aritsune     NaN  Oct 27, 1998      Japan  Jul 20, 2012        4   \n",
       "337147       ariu82    Male  Sep 17, 2003      China  Dec 25, 2020        1   \n",
       "337153    arizima23  Female  Jun 23, 1998      Spain  Mar 15, 2019       15   \n",
       "\n",
       "       completed on_hold dropped plan_to_watch  \n",
       "15           484       5      19            70  \n",
       "19            91      16      14            30  \n",
       "23           380      11      36            62  \n",
       "25           110       0       1            46  \n",
       "29            48       9       5            33  \n",
       "...          ...     ...     ...           ...  \n",
       "337137        16       0       0             0  \n",
       "337138       466       3      48            33  \n",
       "337143        20       3       1            20  \n",
       "337147       228      15      12            29  \n",
       "337153        64       0       4             4  \n",
       "\n",
       "[115390 rows x 10 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>gender</th>\n",
       "      <th>birthday</th>\n",
       "      <th>location</th>\n",
       "      <th>joined</th>\n",
       "      <th>watching</th>\n",
       "      <th>completed</th>\n",
       "      <th>on_hold</th>\n",
       "      <th>dropped</th>\n",
       "      <th>plan_to_watch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>RinsAl</td>\n",
       "      <td>Male</td>\n",
       "      <td>Jul 7, 1989</td>\n",
       "      <td>Japan</td>\n",
       "      <td>Oct 19, 2009</td>\n",
       "      <td>16</td>\n",
       "      <td>484</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ishimori</td>\n",
       "      <td>Female</td>\n",
       "      <td>Apr 17, 2000</td>\n",
       "      <td>France</td>\n",
       "      <td>Dec 7, 2018</td>\n",
       "      <td>1</td>\n",
       "      <td>91</td>\n",
       "      <td>16</td>\n",
       "      <td>14</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Karinnn___</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Feb 28, 2001</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Aug 7, 2018</td>\n",
       "      <td>2</td>\n",
       "      <td>380</td>\n",
       "      <td>11</td>\n",
       "      <td>36</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Rins_</td>\n",
       "      <td>Male</td>\n",
       "      <td>Jul 15, 1988</td>\n",
       "      <td>Japan</td>\n",
       "      <td>Jun 6, 2013</td>\n",
       "      <td>5</td>\n",
       "      <td>110</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>FollowValen</td>\n",
       "      <td>Male</td>\n",
       "      <td>Jul 8, 1996</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Oct 7, 2018</td>\n",
       "      <td>9</td>\n",
       "      <td>48</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337137</th>\n",
       "      <td>arisvgawa</td>\n",
       "      <td>Female</td>\n",
       "      <td>Mar 3, 2001</td>\n",
       "      <td>Japan</td>\n",
       "      <td>Oct 2, 2024</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337138</th>\n",
       "      <td>aritaka</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aug 4</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Apr 24, 2012</td>\n",
       "      <td>18</td>\n",
       "      <td>466</td>\n",
       "      <td>3</td>\n",
       "      <td>48</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337143</th>\n",
       "      <td>aritsune</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Oct 27, 1998</td>\n",
       "      <td>Japan</td>\n",
       "      <td>Jul 20, 2012</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337147</th>\n",
       "      <td>ariu82</td>\n",
       "      <td>Male</td>\n",
       "      <td>Sep 17, 2003</td>\n",
       "      <td>China</td>\n",
       "      <td>Dec 25, 2020</td>\n",
       "      <td>1</td>\n",
       "      <td>228</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337153</th>\n",
       "      <td>arizima23</td>\n",
       "      <td>Female</td>\n",
       "      <td>Jun 23, 1998</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Mar 15, 2019</td>\n",
       "      <td>15</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>115390 rows × 10 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#todo\n",
    "# Just noticed some birthdays have no year but just day and month, how could we manage those?\n",
    "# I guess with errors='coerce', when we change type from object to date, the year is set to a default one (e.g. 1800 or 1900)"
   ],
   "id": "93a1c0ab057e16f1"
  },
  {
   "cell_type": "code",
   "id": "31373a79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T18:06:05.239853Z",
     "start_time": "2025-12-15T18:06:05.222917Z"
    }
   },
   "source": [
    "profiles_df.loc[\n",
    "    profiles_df[\"birthday\"] > profiles_df[\"joined\"],\n",
    "    [\"birthday\", \"joined\"]\n",
    "]\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         birthday     joined\n",
       "2606   2021-07-13 2020-09-14\n",
       "2683   2019-07-11 2017-10-13\n",
       "2817   2077-07-18 2022-01-15\n",
       "4506   2013-09-03 2013-01-26\n",
       "4616   2017-01-12 2015-01-30\n",
       "...           ...        ...\n",
       "333376 2022-09-05 2019-11-22\n",
       "334143 2019-10-11 2012-04-21\n",
       "334335 2020-04-04 2018-12-31\n",
       "335340 2023-09-05 2021-09-12\n",
       "335927 2022-06-15 2016-08-26\n",
       "\n",
       "[284 rows x 2 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>birthday</th>\n",
       "      <th>joined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2606</th>\n",
       "      <td>2021-07-13</td>\n",
       "      <td>2020-09-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2683</th>\n",
       "      <td>2019-07-11</td>\n",
       "      <td>2017-10-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2817</th>\n",
       "      <td>2077-07-18</td>\n",
       "      <td>2022-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4506</th>\n",
       "      <td>2013-09-03</td>\n",
       "      <td>2013-01-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4616</th>\n",
       "      <td>2017-01-12</td>\n",
       "      <td>2015-01-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333376</th>\n",
       "      <td>2022-09-05</td>\n",
       "      <td>2019-11-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334143</th>\n",
       "      <td>2019-10-11</td>\n",
       "      <td>2012-04-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334335</th>\n",
       "      <td>2020-04-04</td>\n",
       "      <td>2018-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335340</th>\n",
       "      <td>2023-09-05</td>\n",
       "      <td>2021-09-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335927</th>\n",
       "      <td>2022-06-15</td>\n",
       "      <td>2016-08-26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284 rows × 2 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "id": "dd2684b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T17:59:00.359899Z",
     "start_time": "2025-12-15T17:59:00.348188Z"
    }
   },
   "source": [
    "profiles_df[\"joined\"].min(), profiles_df[\"joined\"].max()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Timestamp('2004-11-05 00:00:00'), Timestamp('2025-11-02 00:00:00'))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f39e4d",
   "metadata": {},
   "outputs": [],
   "source": "profiles_df[\"birthday\"].dt.year.value_counts().sort_index().head(30)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42635655",
   "metadata": {},
   "outputs": [],
   "source": "profiles_df[profiles_df[\"birthday\"].dt.year == 1930]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa073380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decided to remove the birthdays of people older than 100 when they joined the website\n",
    "mask_too_old = (profiles_df[\"joined\"] - profiles_df[\"birthday\"]).dt.days / 365.25 > 100\n",
    "profiles_df.loc[mask_too_old, \"birthday\"] = pd.NaT\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T18:04:03.164988Z",
     "start_time": "2025-12-15T18:04:03.137712Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mask_too_young = (profiles_df[\"joined\"] - profiles_df[\"birthday\"]).dt.days / 365.25 < 3\n",
    "mask_too_young\n"
   ],
   "id": "90a136f7df882ddf",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         False\n",
       "1         False\n",
       "2         False\n",
       "3         False\n",
       "4         False\n",
       "          ...  \n",
       "337150    False\n",
       "337151    False\n",
       "337152    False\n",
       "337153    False\n",
       "337154    False\n",
       "Length: 337155, dtype: bool"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "id": "7ff1027fe1e58bcf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T18:05:53.834818Z",
     "start_time": "2025-12-15T18:05:53.717091Z"
    }
   },
   "source": [
    "profiles_df.isna().sum()\n",
    "#TODO one guy has null username\n",
    "# We'll put a progressive unkown_user[] label on it, so that for future data with missing username we could go on with it"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "username              1\n",
       "gender           170876\n",
       "birthday         251156\n",
       "location              0\n",
       "joined             1676\n",
       "watching           1678\n",
       "completed          1678\n",
       "on_hold            1678\n",
       "dropped            1678\n",
       "plan_to_watch      1678\n",
       "dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "id": "36fb1286ddcf52d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T18:04:23.586648Z",
     "start_time": "2025-12-15T18:04:23.524413Z"
    }
   },
   "source": [
    "# check if there is any duplicate on \"username\"\n",
    "profiles_df.loc[profiles_df.duplicated(subset=['username'], keep='first')]\n",
    "# none found"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [username, gender, birthday, location, joined, watching, completed, on_hold, dropped, plan_to_watch]\n",
       "Index: []"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>gender</th>\n",
       "      <th>birthday</th>\n",
       "      <th>location</th>\n",
       "      <th>joined</th>\n",
       "      <th>watching</th>\n",
       "      <th>completed</th>\n",
       "      <th>on_hold</th>\n",
       "      <th>dropped</th>\n",
       "      <th>plan_to_watch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "id": "1ad49ed6002b15b0",
   "metadata": {},
   "source": [
    "### THIRTEENTH DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afea57a30ef472",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0c3eddd66192f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c73b87f02bb7ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T16:29:26.196165Z",
     "start_time": "2025-12-04T16:29:26.153263Z"
    }
   },
   "outputs": [],
   "source": [
    "recommendations_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e511bdd1cc10a283",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T16:29:29.076395Z",
     "start_time": "2025-12-04T16:29:28.829775Z"
    }
   },
   "outputs": [],
   "source": [
    "recommendations_df.loc[profiles_df.duplicated(keep='first')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe886e7",
   "metadata": {},
   "source": [
    "# TO KEEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c363c691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st\n",
    "# dropping duplicates\n",
    "character_nicknames_df.drop_duplicates(keep='first', inplace=True)\n",
    "# dropping nan values\n",
    "character_nicknames_df.dropna(inplace=True)\n",
    "\n",
    "# 2nd\n",
    "# no need to clean from duplicates nor missing values\n",
    "# change column types from object to datetime\n",
    "details_df[[\"start_date\", \"end_date\"]] = details_df[[\"start_date\", \"end_date\"]].apply(\n",
    "    pd.to_datetime, errors=\"coerce\"\n",
    ")\n",
    "# change column types from object to Int64\n",
    "details_df[[\"scored_by\", \"rank\", \"episodes\", \"year\"]] = (\n",
    "    details_df[[\"scored_by\", \"rank\", \"episodes\", \"year\"]].astype(\"Int64\")\n",
    ")\n",
    "\n",
    "# 3rd\n",
    "# no need to clean\n",
    "\n",
    "\n",
    "# 4th\n",
    "#dropping nan values because they don't give any useful information\n",
    "person_alternate_names_df.dropna(inplace=True)\n",
    "# dropping duplicates\n",
    "person_alternate_names_df.drop_duplicates(keep='first', inplace=True)\n",
    "\n",
    "\n",
    "# 5th\n",
    "# Dropping duplicates choosing to keep the first occurrence. They only differ for the \"relevant_location\" field which has no interest for us\n",
    "person_details_df.drop_duplicates(subset=['person_mal_id', 'url', 'name'], keep='first', inplace=True)\n",
    "# change column types from object to datetime\n",
    "person_details_df[\"birthday\"] = pd.to_datetime(person_details_df[\"birthday\"], errors='coerce')\n",
    "\n",
    "# There are two weird rows with nan values in most columns but we don't drop them because they may be linked to other datasets\n",
    "# Could we join the two tables person_details_df and person_alternate_names_df having the keys that match.\n",
    "# Putting the alternate names in a new column called alt_name and having a list of those inside\n",
    "# TODO\n",
    "\n",
    "\n",
    "# 6th\n",
    "# no need to clean\n",
    "\n",
    "\n",
    "# 7th\n",
    "# change column types to save memory\n",
    "stats_df.filter(regex=\"_votes$\").astype(\"Int64\")\n",
    "stats_df[stats_df.filter(regex=\"_votes$\").columns] = (\n",
    "    stats_df.filter(regex=\"_votes$\").astype(\"Int64\")\n",
    ")\n",
    "\n",
    "\n",
    "# 8th\n",
    "# change column types to save memory\n",
    "ratings_df[\"is_rewatching\"] = ratings_df[\"is_rewatching\"].astype(\"Int8\")\n",
    "ratings_df[\"anime_id\"] = ratings_df[\"anime_id\"].astype(\"Int32\")\n",
    "# drop duplicates keeping the last entry (most recent)\n",
    "ratings_df.drop_duplicates(subset=['username', 'anime_id'], keep='last', inplace=True)\n",
    "# there's a username with NaN value, we'll keep it for now as it matches with the profiles_df\n",
    "# we only have 1 Nan value in \"username\" in the profiles_df so we can keep it for now\n",
    "# I could manage it this way: generate a deterministic placeholder, e.g.:\n",
    "# the maximum existing user_id + 1\n",
    "# or a specific labeled ID like \"unknown_user\"\n",
    "# THEN we can use this placeholder consistently across all datasets to maintain referential integrity.\n",
    "# profiles_df[\"user_id\"] = profiles_df[\"user_id\"].fillna(new_id)\n",
    "# ratings_df[\"user_id\"] = ratings_df[\"user_id\"].fillna(new_id)\n",
    "# TODO\n",
    "\n",
    "\n",
    "# 9th\n",
    "# change \"character_mal_id\" and \"favorites\" from float to int\n",
    "characters_df[[\"character_mal_id\", \"favorites\"]] = (\n",
    "    characters_df[[\"character_mal_id\", \"favorites\"]].astype(\"Int64\")\n",
    ")\n",
    "\n",
    "# we drop the two rows with all columns Nan\n",
    "characters_df.dropna(how='all', inplace=True)\n",
    "# we drop the duplicates because they have all same values \n",
    "characters_df.drop_duplicates(subset=['character_mal_id', 'url', 'name'], keep='first', inplace=True)\n",
    "\n",
    "\n",
    "# 10th\n",
    "# no need to clean\n",
    "\n",
    "\n",
    "# 11th\n",
    "person_voice_works_df.drop_duplicates(keep='first', inplace=True)\n",
    "\n",
    "\n",
    "# 12th\n",
    "# change column types from object to datetime\n",
    "profiles_df[[\"birthday\", \"joined\"]] = profiles_df[[\"birthday\", \"joined\"]].apply(\n",
    "    pd.to_datetime, errors=\"coerce\"\n",
    ")\n",
    "# setting birthdays outside a reasonable range to NaT\n",
    "profiles_df.loc[\n",
    "    profiles_df[\"birthday\"] > profiles_df[\"joined\"],\n",
    "    \"birthday\"\n",
    "] = pd.NaT\n",
    "# decided to remove the birthdays of people older than 100 at join\n",
    "mask_too_old = (profiles_df[\"joined\"] - profiles_df[\"birthday\"]).dt.days / 365.25 > 100\n",
    "profiles_df.loc[mask_too_old, \"birthday\"] = pd.NaT\n",
    "#I mean MongoDB, a female born in 1930-07-09, from Thailand and joined in 2017-07-09 makes perfect sense, doesn't it?\n",
    "\n",
    "# 13th\n",
    "# no need to clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d983d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### QUESTIONS:\n",
    "1. Can we just leave the commands to clean the dataset instead of leaving all the exploration commands such as .describe, dtypes and so on\n",
    "One file with data cleaning, showing the reasoning.\n",
    "ANOTHER file with Data Visualization.\n",
    "\n",
    "2. Ask for the maximum age in the 5th dataset\n",
    "Justify why you have changed the dates (e.g. change 2070 in 1970) or nan. Possiamo fare il cazzo che vogliamo\n",
    "\n",
    "3. Ask for the only NaN username in the \"ratings\" dataset: for the hci part?\n",
    "Think in terms of the future, take the latest id and sum 1, or progressive uknownuserssssssss (e.g. uknown1, unknown2 in caso poi riceviamo altri dati nel futuro)\n",
    "\n",
    "For TWEB we could just put a random (?) value in order not to lose the line and have non nan values in the database\n",
    "\n",
    "SE NON USIAMO COLONNE IN ASSOLUTO, per alcuna query o nel sito tweb\n",
    "Siccome non faremo mai query su questa cosa qui POSSIAMO TOGLIERE le colonne (e.g. kanji name lo possiamo togliere)"
   ],
   "id": "b72608cba1bf6f29"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "775cebe21ecfa0a9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
